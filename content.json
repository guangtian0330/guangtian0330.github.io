{"posts":[{"title":"Audio streaming on Android:AudioTrack to AHAL","text":"","link":"/2023/09/18/Audio-streaming-on-Android-AudioTrack-to-AHAL/"},{"title":"How AudioServer is initialized on Android","text":"","link":"/2023/09/18/How-AudioServer-is-initialized-on-Android/"},{"title":"Dynamic Programming","text":"","link":"/2023/09/18/Dynamic-Programming/"},{"title":"KNN mechanism for machine learning","text":"","link":"/2023/09/18/KNN-mechanism-for-machine-learning/"},{"title":"LU CS Workshop built","text":"Abstract: This is to memorize the first time we built this team for communication and presentation. THIS is the first conference note for our workshop, here we have the following conclusions: Investigate chances for remote jobs on indeed and linkedin. Investigate popular technology stack on indeed and linkedin. Make a detailed plan for long term career. Decide a second organizer for this workshop.","link":"/2023/09/15/LU-CS-Workshop-built/"},{"title":"Pytorch Introduction","text":"PyTorch Introduction – Tensor1. What is PyTorchPyTorch is a Python-based scientific computing package, which can be used to: Replace NumPy to use the power of GPUs and other accelerators; Using the GPU for deep learning computations can significantly speed up the training process because the GPU is designed to handle parallel computations, which are common in deep learning tasks. However, it’s important to note that not all operations are automatically faster on the GPU. Some operations might actually be slower due to the overhead of transferring data between the CPU and GPU. Typically, **large matrix multiplications **and other computationally intensive operations benefit the most from GPU acceleration. support automatic differentiation functions to implement neural networks. 1. Differences between PyTorch and TensorFlowPyTorch and TensorFlow are two of the most popular deep learning frameworks, and while they share many similarities, there are also some key differences between them. Here are some of the main differences: Dynamic vs. Static Computational Graphs: PyTorch: Uses dynamic computational graphs. The graph is built on-the-fly as operations are executed. This makes it more flexible and intuitive for tasks that involve dynamic or varying computational graphs. TensorFlow: Historically used static computational graphs (TensorFlow 1.x), but with the introduction of TensorFlow 2.x, eager execution is the default, making it more dynamic like PyTorch. However, static graphs can still be constructed using the tf.function decorator. API Design: PyTorch: Has a more Pythonic and imperative API. Many operations can be performed using standard Python control flow constructs, making it easier to understand and debug. TensorFlow: Has a more verbose API, especially in TensorFlow 1.x. TensorFlow 2.x with eager execution is more similar to PyTorch in terms of usability. Model Deployment: PyTorch: Generally considered more user-friendly for model development and research. Deployment tools like TorchScript and TorchServe are available for deploying PyTorch models. TensorFlow: Has a strong focus on production deployment. TensorFlow Serving, TensorFlow Lite, and TensorFlow.js provide tools for deploying models on various platforms. Community and Ecosystem: PyTorch: Has gained popularity in the research community, particularly in domains like computer vision and natural language processing. It has strong support from academia and is often chosen for its ease of use in research. TensorFlow: Has a broader adoption in both research and industry. It has a larger user base and a well-established ecosystem, making it a popular choice for production deployment and large-scale machine learning. Visualization Tools: PyTorch: Uses tools like TensorBoard for visualization, similar to TensorFlow. However, the integration is not as tight as in TensorFlow. TensorFlow: TensorBoard is a well-integrated visualization tool that allows monitoring of various aspects of the training process, graph visualization, and more. Community Support and Documentation: PyTorch: Known for its clean and easy-to-understand documentation. Has a growing and active community. TensorFlow: Has extensive documentation and a well-established community. TensorFlow’s community is often praised for its contributions and support. Eager Execution: PyTorch: Eager execution is the default behavior, allowing for immediate evaluation of operations. This is conducive to interactive and exploratory development. TensorFlow: In TensorFlow 2.x, eager execution is the default behavior, but you can still enable graph execution for performance optimizations. Integration with Other Libraries: PyTorch: Has seamless integration with popular libraries like NumPy, making it easy to switch between PyTorch and NumPy operations. TensorFlow: TensorFlow has its own ecosystem and integrated tools. It has strong support for serving models with TensorFlow Serving and supports TensorFlow Lite for mobile and edge devices. 2. What is TensorTensors are a data structure similar to Numpy’s ndarrays and matrices. It’s used to encode the inputs and outputs, as well as models’ parameters. Tensors can run on GPUs or other specialized hardware to accelerate computing. 2.1 Initialization of TensorThere are 4 ways commonly used to initialize the Tensor: Directly from data, so the data type is automatically inferred. 12data = [[1, 2], [2, 3]]x_data = torch.tensor(data) From a NumPy array a) NumPy array to Tensor 12array_data = np.array(data)tensor_data = torch.from_numpy(array_data) b) Tensor to NumPy array 12tensor_ones = torch.ones(5)numpy_data = tensor_ones.numpy() From another tensor The new tensor retains the properties(shape, datatype) of the argument tensor, unless explicitly overridden. 12x_ones = torch.ones_like(x_data) # retains the properties of x_data but values are changed to one.x_rand = torch.rand_like(x_data, dtype=torch.float) #overrides the datatype of x_data, and values are changed. Output will be like: 1234567Ones Tensor: tensor([[1, 1], [1, 1]])Random Tensor: tensor([[0.8823, 0.9150], [0.3829, 0.9593]]) With random or constant values shape is a tuple of tensor dimensions. 12345678shape = (2, 3,) # The shape determines the dimensionality of the tensorrand_tensor = torch.rand(shape)ones_tensor = torch.ones(shape)zeros_tensor = torch.zeros(shape)print(f&quot;Random Tensor: \\n {rand_tensor} \\n&quot;)print(f&quot;Ones Tensor: \\n {ones_tensor} \\n&quot;)print(f&quot;Zeros Tensor: \\n {zeros_tensor}&quot;) 1234567891011Random Tensor: tensor([[0.3904, 0.6009, 0.2566], [0.7936, 0.9408, 0.1332]])Ones Tensor: tensor([[1., 1., 1.], [1., 1., 1.]])Zeros Tensor: tensor([[0., 0., 0.], [0., 0., 0.]]) 2.2 Tensor AttributesTensor has attributes as shape, datatype, and the device they are running on: tensor.shape, tensor.dtype, tensor.device. 2.3 Operations of Tensor Moving to GPU tensor.to('cuda'): a PyTorch method that is used to move a tensor to a specific device. In this case, it’s moving the tensor to the GPU. The argument 'cuda' refers to the CUDA device, which is the parallel computing architecture developed by NVIDIA that is commonly used for deep learning. 1234# We move our tensor to the GPU if availableif torch.cuda.is_available(): tensor = tensor.to('cuda') print(f&quot;Device tensor is stored on: {tensor.device}&quot;) Indexing and Slicing 123tensor = torch.ones(4, 4)tensor[:,1] = 0 # This will replace the second column's values with 1. print(tensor) 1234tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) Joining tensors You can use torch.cat to concatenate a sequence of tensors along a given dimension. 12t1 = torch.cat([tensor, tensor, tensor], dim=1)print(t1) 1234tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]]) Another way to join tensors is to use torch.stack. This need all tensors to be of the same size. 1torch.stack(tensors, dim=0, *, out=None) → Tensor 12345678# Create some tensorstensor1 = torch.tensor([1, 2, 3])tensor2 = torch.tensor([4, 5, 6])# Stack the tensors along a new dimension (default is dim=0)stacked_tensor = torch.stack([tensor1, tensor2])print(stacked_tensor) 12tensor([[1, 2, 3], [4, 5, 6]]) Multiplying tensors Multiplying can be element-wise**(Hadamard product)** and matrix wise**(Matrix product)**. element-wise: 1234# This computes the element-wise productprint(f&quot;tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n&quot;)# Alternative syntax:print(f&quot;tensor * tensor \\n {tensor * tensor}&quot;) 1234567891011tensor.mul(tensor) tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]])tensor * tensor tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) matrix-wise: 123print(f&quot;tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n&quot;)# Alternative syntax:print(f&quot;tensor @ tensor.T \\n {tensor @ tensor.T}&quot;) 1234567891011tensor.matmul(tensor.T) tensor([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]])tensor @ tensor.T tensor([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) In-place operations In-place and out-of-place: In-place will change the tensor immediately without creating a new one(memory-efficient); out-of-place will create a new tensor with the result and leave the original tensor unchanged(less memory-efficient). 123print(tensor, &quot;\\n&quot;)tensor.add_(5)print(tensor) 123456789tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]])tensor([[6., 5., 6., 6.], [6., 5., 6., 6.], [6., 5., 6., 6.], [6., 5., 6., 6.]])","link":"/2023/11/23/Pytorch-Introduction--%20Tensor/"}],"tags":[{"name":"Audio","slug":"Audio","link":"/tags/Audio/"},{"name":"Android","slug":"Android","link":"/tags/Android/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"Andorid","slug":"Andorid","link":"/tags/Andorid/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"workshop","slug":"workshop","link":"/tags/workshop/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"}],"categories":[{"name":"Android","slug":"Android","link":"/categories/Android/"},{"name":"algorithm","slug":"algorithm","link":"/categories/algorithm/"},{"name":"journal","slug":"journal","link":"/categories/journal/"}],"pages":[{"title":"404","text":"404 Sorry, the page that you're looking for does not exist. You could visit the homepage If you've just bumped into a broken link, please report it to info@yourdomain.com.","link":"/404.html"}]}