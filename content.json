{"posts":[{"title":"Dynamic Programming","text":"","link":"/2023/09/18/Dynamic-Programming/"},{"title":"Audio streaming on Android:AudioTrack to AHAL","text":"","link":"/2023/09/18/Audio-streaming-on-Android-AudioTrack-to-AHAL/"},{"title":"LU CS Workshop built","text":"Abstract: This is to memorize the first time we built this team for communication and presentation. THIS is the first conference note for our workshop, here we have the following conclusions: Investigate chances for remote jobs on indeed and linkedin. Investigate popular technology stack on indeed and linkedin. Make a detailed plan for long term career. Decide a second organizer for this workshop.","link":"/2023/09/15/LU-CS-Workshop-built/"},{"title":"How AudioServer is initialized on Android","text":"","link":"/2023/09/18/How-AudioServer-is-initialized-on-Android/"},{"title":"KNN mechanism for machine learning","text":"","link":"/2023/09/18/KNN-mechanism-for-machine-learning/"},{"title":"Pytorch Introduction--Tensor","text":"1. What is PyTorchPyTorch is a Python-based scientific computing package, which can be used to: Replace NumPy to use the power of GPUs and other accelerators; Using the GPU for deep learning computations can significantly speed up the training process because the GPU is designed to handle parallel computations, which are common in deep learning tasks. However, it’s important to note that not all operations are automatically faster on the GPU. Some operations might actually be slower due to the overhead of transferring data between the CPU and GPU. Typically, large matrix multiplications and other computationally intensive operations benefit the most from GPU acceleration. support automatic differentiation functions to implement neural networks. 2. What is TensorTensors are a data structure similar to Numpy’s ndarrays and matrices. It’s used to encode the inputs and outputs, as well as models’ parameters. Tensors can run on GPUs or other specialized hardware to accelerate computing. 2.1 Initialization of TensorThere are 4 ways commonly used to initialize the Tensor: Directly from data, so the data type is automatically inferred. 12data = [[1, 2], [2, 3]]x_data = torch.tensor(data) From a NumPy array a) NumPy array to Tensor 12array_data = np.array(data)tensor_data = torch.from_numpy(array_data) b) Tensor to NumPy array 12tensor_ones = torch.ones(5)numpy_data = tensor_ones.numpy() From another tensor The new tensor retains the properties(shape, datatype) of the argument tensor, unless explicitly overridden. 12x_ones = torch.ones_like(x_data) # retains the properties of x_data but values are changed to one.x_rand = torch.rand_like(x_data, dtype=torch.float) #overrides the datatype of x_data, and values are changed. Output will be like: 1234567Ones Tensor: tensor([[1, 1], [1, 1]])Random Tensor: tensor([[0.8823, 0.9150], [0.3829, 0.9593]]) With random or constant values shape is a tuple of tensor dimensions. 12345678shape = (2, 3,) # The shape determines the dimensionality of the tensorrand_tensor = torch.rand(shape)ones_tensor = torch.ones(shape)zeros_tensor = torch.zeros(shape)print(f&quot;Random Tensor: \\n {rand_tensor} \\n&quot;)print(f&quot;Ones Tensor: \\n {ones_tensor} \\n&quot;)print(f&quot;Zeros Tensor: \\n {zeros_tensor}&quot;) 1234567891011Random Tensor: tensor([[0.3904, 0.6009, 0.2566], [0.7936, 0.9408, 0.1332]])Ones Tensor: tensor([[1., 1., 1.], [1., 1., 1.]])Zeros Tensor: tensor([[0., 0., 0.], [0., 0., 0.]]) 2.2 Tensor AttributesTensor has attributes as shape, datatype, and the device they are running on: tensor.shape, tensor.dtype, tensor.device. 2.3 Operations of Tensor Moving to GPU tensor.to('cuda'): a PyTorch method that is used to move a tensor to a specific device. In this case, it’s moving the tensor to the GPU. The argument 'cuda' refers to the CUDA device, which is the parallel computing architecture developed by NVIDIA that is commonly used for deep learning. 1234# We move our tensor to the GPU if availableif torch.cuda.is_available(): tensor = tensor.to('cuda') print(f&quot;Device tensor is stored on: {tensor.device}&quot;) Indexing and Slicing 123tensor = torch.ones(4, 4)tensor[:,1] = 0 # This will replace the second column's values with 1. print(tensor) 1234tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) Joining tensors You can use torch.cat to concatenate a sequence of tensors along a given dimension. 12t1 = torch.cat([tensor, tensor, tensor], dim=1)print(t1) 1234tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.], [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]]) Another way to join tensors is to use torch.stack. This need all tensors to be of the same size. 1torch.stack(tensors, dim=0, *, out=None) → Tensor 12345678# Create some tensorstensor1 = torch.tensor([1, 2, 3])tensor2 = torch.tensor([4, 5, 6])# Stack the tensors along a new dimension (default is dim=0)stacked_tensor = torch.stack([tensor1, tensor2])print(stacked_tensor) 12tensor([[1, 2, 3], [4, 5, 6]]) Multiplying tensors Multiplying can be element-wise**(Hadamard product)** and matrix wise**(Matrix product)**. element-wise: 1234# This computes the element-wise productprint(f&quot;tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n&quot;)# Alternative syntax:print(f&quot;tensor * tensor \\n {tensor * tensor}&quot;) 1234567891011tensor.mul(tensor) tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]])tensor * tensor tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) matrix-wise: 123print(f&quot;tensor.matmul(tensor.T) \\n {tensor.matmul(tensor.T)} \\n&quot;)# Alternative syntax:print(f&quot;tensor @ tensor.T \\n {tensor @ tensor.T}&quot;) 1234567891011tensor.matmul(tensor.T) tensor([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]])tensor @ tensor.T tensor([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) In-place operations In-place and out-of-place: In-place will change the tensor immediately without creating a new one(memory-efficient); out-of-place will create a new tensor with the result and leave the original tensor unchanged(less memory-efficient). 123print(tensor, &quot;\\n&quot;)tensor.add_(5)print(tensor) 123456789tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]])tensor([[6., 5., 6., 6.], [6., 5., 6., 6.], [6., 5., 6., 6.], [6., 5., 6., 6.]])","link":"/2023/11/23/Pytorch-Introduction/"}],"tags":[{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"Audio","slug":"Audio","link":"/tags/Audio/"},{"name":"Android","slug":"Android","link":"/tags/Android/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"workshop","slug":"workshop","link":"/tags/workshop/"},{"name":"Andorid","slug":"Andorid","link":"/tags/Andorid/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"}],"categories":[{"name":"algorithm","slug":"algorithm","link":"/categories/algorithm/"},{"name":"Android","slug":"Android","link":"/categories/Android/"},{"name":"journal","slug":"journal","link":"/categories/journal/"}],"pages":[{"title":"404","text":"404 Sorry, the page that you're looking for does not exist. You could visit the homepage If you've just bumped into a broken link, please report it to info@yourdomain.com.","link":"/404.html"}]}